{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from concrete.ml.torch.compile import compile_torch_model\n",
    "from concrete.ml.quantization.quantized_module import QuantizedModule\n",
    "from concrete.ml.torch.compile import compile_brevitas_qat_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "no_epochs = 2\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the dataset into tensors normalized range [-1, 1]\n",
    "transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5),(0.5))     \n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data sets downloading and reading\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                        train=True,\n",
    "                                        download=True,\n",
    "                                        transform=transform\n",
    "                                        )\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data',\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform=transform\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = load_digits(return_X_y=True)\n",
    "\n",
    "# X = np.expand_dims(X.reshape((-1, 8, 8)), 1)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.25, shuffle=True, random_state=42\n",
    "# )\n",
    "\n",
    "# print(x_train.dtype)\n",
    "# print(x_train.shape)\n",
    "# print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "(2000, 1, 28, 28)\n",
      "Shape of x_train from MNIST: (2000, 1, 28, 28)\n",
      "Shape of x_test from MNIST: (800, 1, 28, 28)\n",
      "Shape of y_train from MNIST: (2000,)\n",
      "Shape of y_test from MNIST: (800,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract features (images) and labels from MNIST dataset\n",
    "mnist_features = train_dataset.data.numpy().reshape(-1, 28, 28)\n",
    "mnist_labels = train_dataset.targets.numpy()\n",
    "\n",
    "# Reshape and expand dimensions to match the structure of load_digits dataset\n",
    "x_train_mnist = np.expand_dims(mnist_features, 1)\n",
    "\n",
    "# Split the MNIST data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_train_mnist, mnist_labels,  train_size=5000, test_size=800, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "x_train = x_train.astype('float64')\n",
    "print(x_train.dtype)\n",
    "print(x_train.shape)\n",
    "\n",
    "# print(x_train)\n",
    "# plt.imshow(x_train[0,0], cmap='grey')\n",
    "# plt.show()\n",
    "# Verify the shapes\n",
    "print(\"Shape of x_train from MNIST:\", x_train.shape)\n",
    "print(\"Shape of x_test from MNIST:\", x_test.shape)\n",
    "print(\"Shape of y_train from MNIST:\", y_train.shape)\n",
    "print(\"Shape of y_test from MNIST:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataloaders - used to load data into application for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True\n",
    "                            )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "we need to spilt the data here and \n",
    "convert it into a format that can be used with compile_torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# class ConvolutionalNueralNet(nn.Module):\n",
    "#     def __init__(self, n_classes) -> None:\n",
    "#         \"\"\"Construct the CNN with a configurable number of classes.\"\"\"\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "#         self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "#         self.conv3 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "#         self.fc1 = nn.Linear(64 * 3 * 3, 64)\n",
    "#         self.fc2 = nn.Linear(64, n_classes)\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x= self.conv1(x)\n",
    "#         x = functional.relu(x)\n",
    "#         x = self.pool(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = functional.relu(x)\n",
    "#         x = self.pool(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = functional.relu(x)\n",
    "#         x = self.flatten\n",
    "#         # x = x.view(-1, 64 * 3 * 3)\n",
    "#         x = self.fc1(x)\n",
    "#         x = functional.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "    \n",
    "# summary(ConvolutionalNueralNet(10), (1,28,28), device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 26, 26]             320\n",
      "            Conv2d-2           [-1, 64, 24, 24]          18,496\n",
      "            Conv2d-3             [-1, 64, 8, 8]          36,928\n",
      "            Linear-4                   [-1, 64]         262,208\n",
      "            Linear-5                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 318,602\n",
      "Trainable params: 318,602\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.48\n",
      "Params size (MB): 1.22\n",
      "Estimated Total Size (MB): 1.70\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "class ConvolutionalNueralNet(nn.Module):\n",
    "    def __init__(self, n_classes) -> None:\n",
    "        \"\"\"Construct the CNN with a configurable number of classes.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=3)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 64)\n",
    "        self.fc2 = nn.Linear(64, n_classes)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.relu(x)\n",
    "        # x = self.flatten(x)\n",
    "        x = x.flatten(1)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "summary(ConvolutionalNueralNet(10), (1,28,28), device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TinyCNN(nn.Module):\n",
    "#     \"\"\"A very small CNN to classify the sklearn digits data-set.\"\"\"\n",
    "\n",
    "#     def __init__(self, n_classes) -> None:\n",
    "#         \"\"\"Construct the CNN with a configurable number of classes.\"\"\"\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Adjusted the architecture for MNIST\n",
    "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "#         self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "#         self.fc1 = nn.Linear(64 * 7 * 7, 128) \n",
    "#         self.fc2 = nn.Linear(128, n_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.conv1(x))\n",
    "#         x = self.pool(x)\n",
    "#         x = torch.relu(self.conv2(x))\n",
    "#         x = self.pool(x)\n",
    "#         x = torch.relu(self.conv3(x))\n",
    "#         x = x.view(-1, 64 * 7 * 7)\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvolutionalNueralNet(10).to(device)\n",
    "# net = TinyCNN(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:54<00:00, 27.24s/it]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "def train_one_epoch(net, optimizer, train_loader):\n",
    "    # Cross Entropy loss for classification when not using a softmax layer in the network\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    net.train()\n",
    "    avg_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss_net = loss(output, target.long())\n",
    "        loss_net.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss_net.item()\n",
    "\n",
    "    return avg_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# Create the tiny CNN with 10 output classes\n",
    "N_EPOCHS = no_epochs\n",
    "\n",
    "# Train the network with Adam, output the test set accuracy every epoch\n",
    "losses_bits = []\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "for _ in tqdm(range(N_EPOCHS), desc=\"Training\"):\n",
    "    losses_bits.append(train_one_epoch(net, optimizer, train_loader))\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 4))\n",
    "# plt.plot(losses_bits)\n",
    "# plt.ylabel(\"Cross Entropy Loss\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.title(\"Training set loss during training\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Work now with concrete\n",
    "\n",
    "def test_with_concrete(quantized_module, test_loader, use_sim):\n",
    "    \"\"\"Test a neural network that is quantized and compiled with Concrete ML.\"\"\"\n",
    "\n",
    "    # Casting the inputs into int64 is recommended\n",
    "    all_y_pred = np.zeros((len(test_loader)), dtype=np.int64)\n",
    "    all_targets = np.zeros((len(test_loader)), dtype=np.int64)\n",
    "\n",
    "    # Iterate over the test batches and accumulate predictions and ground truth labels in a vector\n",
    "    idx = 0\n",
    "    for data, target in tqdm(test_loader):\n",
    "        data = data.numpy()\n",
    "        target = target.numpy()\n",
    "\n",
    "        fhe_mode = \"simulate\" if use_sim else \"execute\"\n",
    "\n",
    "        # Quantize the inputs and cast to appropriate data type\n",
    "        y_pred = quantized_module.forward(data, fhe=fhe_mode)\n",
    "\n",
    "        endidx = idx + target.shape[0]\n",
    "\n",
    "        # Accumulate the ground truth labels\n",
    "        all_targets[idx:endidx] = target\n",
    "\n",
    "        # Get the predicted class id and accumulate the predictions\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        all_y_pred[idx:endidx] = y_pred\n",
    "\n",
    "        # Update the index\n",
    "        idx += target.shape[0]\n",
    "\n",
    "    # Compute and report results\n",
    "    n_correct = np.sum(all_targets == all_y_pred)\n",
    "\n",
    "    return n_correct / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float64')\n",
    "# print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't compile: Cannot find crypto parameters",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m n_bits \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[0;32m----> 3\u001b[0m q_module \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_torch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrounding_threshold_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# p_error=0.1, \u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# verbose=True,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# configuration=None\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#q_module = compile_brevitas_qat_model(net, x_train)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# start_time = time.time()\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# accs = test_with_concrete(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print(f\"Simulated FHE execution for {n_bits} bit network accuracy: {accs:.2f}%\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concrete/ml/torch/compile.py:271\u001b[0m, in \u001b[0;36mcompile_torch_model\u001b[0;34m(torch_model, torch_inputset, import_qat, configuration, artifacts, show_mlir, n_bits, rounding_threshold_bits, p_error, global_p_error, verbose, inputs_encryption_status)\u001b[0m\n\u001b[1;32m    259\u001b[0m assert_true(\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(torch_model, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule),\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe compile_torch_model function must be called on a torch.nn.Module\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    262\u001b[0m )\n\u001b[1;32m    264\u001b[0m assert_false(\n\u001b[1;32m    265\u001b[0m     has_any_qnn_layers(torch_model),\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe compile_torch_model was called on a torch.nn.Module that contains \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBrevitas quantized layers. These models must be imported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing compile_brevitas_qat_model instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    269\u001b[0m )\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_torch_or_onnx_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_inputset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimport_qat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_mlir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_mlir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_bits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrounding_threshold_bits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrounding_threshold_bits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_p_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_p_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_encryption_status\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_encryption_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concrete/ml/torch/compile.py:200\u001b[0m, in \u001b[0;36m_compile_torch_or_onnx_model\u001b[0;34m(model, torch_inputset, import_qat, configuration, artifacts, show_mlir, n_bits, rounding_threshold_bits, p_error, global_p_error, verbose, inputs_encryption_status)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Find the right way to set parameters for compiler, depending on the way we want to default\u001b[39;00m\n\u001b[1;32m    198\u001b[0m p_error, global_p_error \u001b[38;5;241m=\u001b[39m manage_parameters_for_pbs_errors(p_error, global_p_error)\n\u001b[0;32m--> 200\u001b[0m \u001b[43mquantized_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputset_as_numpy_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_mlir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_mlir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_p_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_p_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_encryption_status\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_encryption_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m quantized_module\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concrete/ml/quantization/quantized_module.py:680\u001b[0m, in \u001b[0;36mQuantizedModule.compile\u001b[0;34m(self, inputs, configuration, artifacts, show_mlir, p_error, global_p_error, verbose, inputs_encryption_status)\u001b[0m\n\u001b[1;32m    676\u001b[0m p_error, global_p_error \u001b[38;5;241m=\u001b[39m manage_parameters_for_pbs_errors(p_error, global_p_error)\n\u001b[1;32m    678\u001b[0m \u001b[38;5;66;03m# Jit compiler is now deprecated and will soon be removed, it is thus forced to False\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;66;03m# by default\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfhe_circuit \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_mlir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_mlir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_p_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_p_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43msingle_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfhe_simulation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfhe_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_compiled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfhe_circuit\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concrete/fhe/compilation/compiler.py:538\u001b[0m, in \u001b[0;36mCompiler.compile\u001b[0;34m(self, inputset, configuration, artifacts, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m columns)\n\u001b[0;32m--> 538\u001b[0m circuit \u001b[38;5;241m=\u001b[39m \u001b[43mCircuit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlir_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompilation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(circuit, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    546\u001b[0m     client_parameters \u001b[38;5;241m=\u001b[39m circuit\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mspecs\u001b[38;5;241m.\u001b[39mclient_parameters\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concrete/fhe/compilation/circuit.py:62\u001b[0m, in \u001b[0;36mCircuit.__init__\u001b[0;34m(self, graph, mlir, compilation_context, configuration)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_fhe_simulation()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfiguration\u001b[38;5;241m.\u001b[39mfhe_execution:\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_fhe_execution\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concrete/fhe/compilation/circuit.py:95\u001b[0m, in \u001b[0;36mCircuit.enable_fhe_execution\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03mEnable FHE execution.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserver\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver \u001b[38;5;241m=\u001b[39m \u001b[43mServer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlir_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompilation_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompilation_context\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     keyset_cache_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfiguration\u001b[38;5;241m.\u001b[39muse_insecure_key_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concrete/fhe/compilation/server.py:194\u001b[0m, in \u001b[0;36mServer.create\u001b[0;34m(mlir, configuration, is_simulated, compilation_context)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# MlirModule\u001b[39;00m\n\u001b[1;32m    191\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    192\u001b[0m                 compilation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    193\u001b[0m             ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust provide compilation context when compiling MlirModule\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 194\u001b[0m             compilation_result \u001b[38;5;241m=\u001b[39m \u001b[43msupport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompilation_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m         server_lambda \u001b[38;5;241m=\u001b[39m support\u001b[38;5;241m.\u001b[39mload_server_lambda(compilation_result)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concrete/compiler/library_support.py:170\u001b[0m, in \u001b[0;36mLibrarySupport.compile\u001b[0;34m(self, mlir_program, options, compilation_context)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    165\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompilation_context must be of type CompilationContext, not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(compilation_context)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         )\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LibraryCompilationResult\u001b[38;5;241m.\u001b[39mwrap(\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmlir_program\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_CAPIPtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompilation_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m     )\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# pylint: enable=protected-access\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LibraryCompilationResult\u001b[38;5;241m.\u001b[39mwrap(\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpp()\u001b[38;5;241m.\u001b[39mcompile(mlir_program, options\u001b[38;5;241m.\u001b[39mcpp())\n\u001b[1;32m    177\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't compile: Cannot find crypto parameters"
     ]
    }
   ],
   "source": [
    "n_bits = 6\n",
    "\n",
    "q_module = compile_torch_model(\n",
    "    net, \n",
    "    x_train, \n",
    "    rounding_threshold_bits=6, \n",
    "    # p_error=0.1, \n",
    "    # verbose=True,\n",
    "    # configuration=None\n",
    ")\n",
    "#q_module = compile_brevitas_qat_model(net, x_train)\n",
    "# start_time = time.time()\n",
    "# accs = test_with_concrete(\n",
    "#     q_module,\n",
    "#     train_loader,\n",
    "#     use_sim=True,\n",
    "# )\n",
    "# sim_time = time.time() - start_time\n",
    "\n",
    "# print(f\"Simulated FHE execution for {n_bits} bit network accuracy: {accs:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q_module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate keys first\u001b[39;00m\n\u001b[1;32m      2\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mq_module\u001b[49m\u001b[38;5;241m.\u001b[39mfhe_circuit\u001b[38;5;241m.\u001b[39mkeygen()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeygen time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'q_module' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate keys first\n",
    "t = time.time()\n",
    "q_module.fhe_circuit.keygen()\n",
    "print(f\"Keygen time: {time.time()-t:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t = time.time()\n",
    "accuracy_test = test_with_concrete(\n",
    "    q_module,\n",
    "    test_loader,\n",
    "    use_sim=False,\n",
    ")\n",
    "elapsed_time = time.time() - t\n",
    "time_per_inference = elapsed_time / len(test_loader)\n",
    "accuracy_percentage = 100 * accuracy_test\n",
    "\n",
    "print(\n",
    "    f\"Time per inference in FHE: {time_per_inference:.2f} \"\n",
    "    f\"with {accuracy_percentage:.2f}% accuracy\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
