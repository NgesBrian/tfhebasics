{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/ASURITE/nnjungle/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from concrete.ml.torch.compile import compile_torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "no_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the dataset into tensors normalized range [-1, 1]\n",
    "transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,0.5,0.5),(0.5, 0.5, 0.5))     \n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:03<00:00, 47836831.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#data sets downloading and reading\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', \n",
    "                                        train=True,\n",
    "                                        download=True,\n",
    "                                        transform=transform\n",
    "                                        )\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='data',\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform=transform\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Dataloaders - used to load data into application for use\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True\n",
    "                            )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNueralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3)\n",
    "        self.fc1 = nn.Linear(64*4*4, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = functional.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = functional.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = functional.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionalNueralNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_steps = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 1.451\n",
      "Finished Training\n",
      "[2] loss: 1.085\n",
      "Finished Training\n",
      "[3] loss: 0.932\n",
      "Finished Training\n",
      "[4] loss: 0.832\n",
      "Finished Training\n",
      "[5] loss: 0.760\n",
      "Finished Training\n",
      "[6] loss: 0.703\n",
      "Finished Training\n",
      "[7] loss: 0.655\n",
      "Finished Training\n",
      "[8] loss: 0.613\n",
      "Finished Training\n",
      "[9] loss: 0.572\n",
      "Finished Training\n",
      "[10] loss: 0.541\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(no_epochs):\n",
    "    #loaded_model\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f'[{epoch+1 }] loss: {running_loss/n_total_steps:.3f}')\n",
    "\n",
    "    print('Finished Training')\n",
    "    PATH ='./cnn.pth'\n",
    "    torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 73.19 %\n",
      "Accuracy of the loaded model: 73.19 %\n"
     ]
    }
   ],
   "source": [
    "loaded_model = ConvolutionalNueralNet().to(device)\n",
    "loaded_model.load_state_dict(torch.load(PATH))\n",
    "loaded_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_correct2 = 0\n",
    "    n_samples = len(test_loader.dataset)\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        #max returns (value, index)\n",
    "        _, predicted = torch.max(outputs,1)\n",
    "        n_correct +=(predicted == labels).sum().item()\n",
    "        \n",
    "        outputs2 = loaded_model(images)\n",
    "        _, predicted2 = torch.max(outputs, 1)\n",
    "        n_correct2 +=(predicted2 == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct/ n_samples\n",
    "    print(f'Accuracy of the model: {acc} %')\n",
    "\n",
    "    acc = 100.0 * n_correct2 / n_samples\n",
    "    print(f'Accuracy of the loaded model: {acc} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Work now with concrete\n",
    "\n",
    "def test_with_concrete(quantized_module, test_loader, use_sim):\n",
    "    \"\"\"Test a neural network that is quantized and compiled with Concrete ML.\"\"\"\n",
    "\n",
    "    # Casting the inputs into int64 is recommended\n",
    "    all_y_pred = np.zeros((len(test_loader)), dtype=np.int64)\n",
    "    all_targets = np.zeros((len(test_loader)), dtype=np.int64)\n",
    "\n",
    "    # Iterate over the test batches and accumulate predictions and ground truth labels in a vector\n",
    "    idx = 0\n",
    "    for data, target in tqdm(test_loader):\n",
    "        data = data.numpy()\n",
    "        target = target.numpy()\n",
    "\n",
    "        fhe_mode = \"simulate\" if use_sim else \"execute\"\n",
    "\n",
    "        # Quantize the inputs and cast to appropriate data type\n",
    "        y_pred = quantized_module.forward(data, fhe=fhe_mode)\n",
    "\n",
    "        endidx = idx + target.shape[0]\n",
    "\n",
    "        # Accumulate the ground truth labels\n",
    "        all_targets[idx:endidx] = target\n",
    "\n",
    "        # Get the predicted class id and accumulate the predictions\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        all_y_pred[idx:endidx] = y_pred\n",
    "\n",
    "        # Update the index\n",
    "        idx += target.shape[0]\n",
    "\n",
    "    # Compute and report results\n",
    "    n_correct = np.sum(all_targets == all_y_pred)\n",
    "\n",
    "    return n_correct / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bits = 6\n",
    "\n",
    "q_module = compile_torch_model(model, n_total_steps, rounding_threshold_bits=6, p_error=0.1)\n",
    "\n",
    "start_time = time.time()\n",
    "accs = test_with_concrete(\n",
    "    q_module,\n",
    "    train_loader,\n",
    "    use_sim=True,\n",
    ")\n",
    "sim_time = time.time() - start_time\n",
    "\n",
    "print(f\"Simulated FHE execution for {n_bits} bit network accuracy: {accs:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate keys first\n",
    "t = time.time()\n",
    "q_module.fhe_circuit.keygen()\n",
    "print(f\"Keygen time: {time.time()-t:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t = time.time()\n",
    "accuracy_test = test_with_concrete(\n",
    "    q_module,\n",
    "    test_loader,\n",
    "    use_sim=False,\n",
    ")\n",
    "elapsed_time = time.time() - t\n",
    "time_per_inference = elapsed_time / len(test_loader)\n",
    "accuracy_percentage = 100 * accuracy_test\n",
    "\n",
    "print(\n",
    "    f\"Time per inference in FHE: {time_per_inference:.2f} \"\n",
    "    f\"with {accuracy_percentage:.2f}% accuracy\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
